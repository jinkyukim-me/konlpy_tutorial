{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리 NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리는 인간의 언어 현상을 컴퓨터와 같은 기계를 이용하여 모사할 수 있도록 연구하고 이를 구현하는 인공지능의 주요 분야 중 하나다.<br>\n",
    "컴퓨터 공학과 언어학과 같은 다른 학문과의 융합적인 요소도 필요하며, 컴퓨터와 인간 사이의 연결고리를 만들어주는 것이 자연어 처리 분야라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리의 주 응용분야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sentiment Analysis - 감성분석, 인터넷 이용자의 글에서 감정을 분류하는 작업\n",
    "* Quention & Answering - QA, 지문을 읽고 주어진 질문에 대해 대답하는 분야\n",
    "* 애플의 Siri와 같이 사용자의 의도를 파악하고 대화하거나 도움을 주는 작업\n",
    "* Summarization 요약, Machine Translation 기계번역과 같은 작업\n",
    "* Language Modeling - 언어모델링, 첫 몇 마디만 입력하면 뒤에 나올 단어 예측해서 선택만 하면 되는 스마트보드\n",
    "\n",
    "1950년부터 자연어를 처리하려는 시도가 있었고 그 뒤로 계속 문제점을 가지고 있다가 <br>\n",
    "딥러닝이 발전하면서 2014년에 seq2seq(Sequence-to-Sequence)구조가 제시되면서 기계 번역은 신경망을 기반으로 하는 것이 대세가 됨\n",
    "\n",
    "딥러닝이 처음에 이미지 처리 분야에서 강한 면모를 보였다면 자연어 처리는 최근에 가장 진척이 많이 나간 연구 분야이기도 함<br>\n",
    "최근 모든 자연어 처리 문제에서 최고의 경지인 SOTA(State od the art)를 기록하고 있는 모델은 BERT(Bidirectional Encoder Representations from Transformers) <br> \n",
    "\n",
    "BERT는 \"Attention is All You Need\" 논문에 등장한 Trnasformer 모델의 Encoder 부분을 기반으로 대량의 코퍼스를 통해 비지도학습으로 범용적인 언어 모델을 학습한 뒤에 풀려고 하는 문제(Task)에 전이 학습(Transfer Learning)을 하는 방법임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 자연어 처리의 일반적인 프로세스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에서 자연어 처리를 수행하기 위해서 텍스트 형식의 데이터를 머신러닝 알고리즘이 이해할 수 있는 숫자<br>\n",
    "즉, 벡터 형식의 데이터로 만들어 줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tokenization - 문장의 토큰화\n",
    "2. vocabulary - 중복되지 않은 토근으로 vocabulary 생성 후, 숫자와 1대 1 대응\n",
    "3. numericalization - 단어장(vocabulary)를 기반으로 토큰을 숫자로 전환 \n",
    "4. padding - 길이가 다양한 문장을 동일한 길이로 만들어 주는 padding 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장이란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리에서 제일 우선적으로 해야할 작업은 \n",
    "* 문장(sentence)를 기계가 처리할 수 있는 데이터로 만드는 것\n",
    "\n",
    "문장(sentence)는 길이가 다른 일련의 토큰(token)의 연속이다.\n",
    "* 즉, '순서'라는 특징을 가진 시퀀스 데이터(sequence data)라고 할 수 있다.\n",
    "* 여기서 토큰(token)은 `단어`가 될 수도 있고, `형태소`가 될 수도 있고 `하나의 문자열`이 될 수도 있다.\n",
    "    * 형태소 : 의미를 가지는 요소로서는 더 이상 분석할 수 없는 가장 작은 말의 단위\n",
    "    \n",
    "Example: 문장(sentence) - \"오늘 날씨 좋다\"<br>\n",
    "\n",
    "**tokenization**<br>\n",
    "* token - 단어(띄어쓰기 기준) : '오늘' + '날씨' + '좋다'\n",
    "* token - 형태소 : '오늘 ' + '날씨' + '좋' + '다'\n",
    "* token - 문자열 : '오' + '늘' + ' ' + '날' + '씨' + ' ' + '좋' + '다'\n",
    "\n",
    "**중요한 포인트**<br>\n",
    "영어의 경우 띄어쓰기로 단어 위주의 토큰화를 할 수 있다.(nltk, SpaCy)(띄어쓰기 기준 split)\n",
    "한글의 경우 띄어쓰기로 토큰화할 경우 단어 수가 많아지기 때문에 형태소 단위의 토큰화를 기본으로 한다.(KoNLPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:49:21.439016Z",
     "start_time": "2019-09-29T08:49:21.435024Z"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
