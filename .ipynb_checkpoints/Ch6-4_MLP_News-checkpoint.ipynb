{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 학습 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.359387Z",
     "start_time": "2019-09-28T00:36:52.704854Z"
    }
   },
   "outputs": [],
   "source": [
    "# OS 라이브러리 임포트\n",
    "import os\n",
    "# 정규표현식 라이브러리 임포트\n",
    "import re\n",
    "\n",
    "# scikit-learn 라이브러리 임포트\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 형태소분석기 라이브러리\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "# pandas 라이브러리 임포트\n",
    "import pandas as pd\n",
    "\n",
    "# numpy 라이브러리 임포트\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.367335Z",
     "start_time": "2019-09-28T00:36:54.361772Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_prefix = '../data/'\n",
    "target_dir = 'HKIB-20000'\n",
    "cat_dirs = ['health', 'economy', 'science', 'education', 'culture', 'society', 'industry', 'leisure', 'politics']\n",
    "cat_prefixes = ['건강', '경제', '과학', '교육', '문화', '사회', '산업', '여가', '정치']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.664694Z",
     "start_time": "2019-09-28T00:36:54.369922Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: '../data/HKIB-20000'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-95706d27c9b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 데이터 정리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtarget_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 5분할된 텍스트 파일을 각각 처리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: '../data/HKIB-20000'"
     ]
    }
   ],
   "source": [
    "# 데이터 정리\n",
    "files = os.listdir(dir_prefix + target_dir)\n",
    "\n",
    "# 5분할된 텍스트 파일을 각각 처리\n",
    "for file in files:\n",
    "    # 데이터가 담긴 파일만 처리\n",
    "    if not file.endswith('.txt'):\n",
    "        continue\n",
    "    \n",
    "    # 각 텍스트 파일을 처리\n",
    "    with open(dir_prefix + target_dir + '/' + file) as currfile:\n",
    "        doc_cnt = 0\n",
    "        docs = []\n",
    "        curr_doc = None\n",
    "        \n",
    "        # 기사 단위로 분할하여 리스트를 생성\n",
    "        for curr_line in currfile:\n",
    "            if curr_line.startswith('@DOCUMENT'):\n",
    "                if curr_doc is not None:\n",
    "                    docs.append(curr_doc)\n",
    "                curr_doc = curr_line\n",
    "                doc_cnt = doc_cnt + 1\n",
    "                continue\n",
    "            curr_doc = curr_doc + curr_line\n",
    "        \n",
    "        # 각 기사를 대주제 별로 분류하여 기사별 파일로 정리\n",
    "        for doc in docs:\n",
    "            doc_lines = doc.split('\\n')\n",
    "            doc_no = doc_lines[1][9:]\n",
    "            \n",
    "            # 주제 추출\n",
    "            doc_cat03 = ''\n",
    "            for line in doc_lines[:10]:\n",
    "                if line.startswith(\"#CAT'03:\"):\n",
    "                    doc_cat03 = line[10:]\n",
    "                    break\n",
    "            \n",
    "            # 추출한 주제 별로 디렉토리 정리\n",
    "            for cat_prefix in cat_prefixes:\n",
    "                if doc_cat03.startswith(cat_prefix):\n",
    "                    dir_index = cat_prefixes.index(cat_prefix)\n",
    "                    break\n",
    "                    \n",
    "            # 문서 정보를 제거하고 기사 본문만 남기기\n",
    "            filtered_lines = []\n",
    "            for line in doc_lines:\n",
    "                if not (line.startswith('#') or line.startswith('@')):\n",
    "                    filtered_lines.append(line)\n",
    "                    \n",
    "            # 주제별 디렉토리에 기사를 파일로 쓰기\n",
    "            filename = 'hkib-' + doc_no + '.txt'\n",
    "            filepath = dir_prefix + target_dir + '/' + cat_dirs[dir_index]\n",
    "            \n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            f = open(filepath + '/' + filename, 'w')\n",
    "            f.write('\\n'.join(filtered_lines))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.669559Z",
     "start_time": "2019-09-28T00:36:52.716Z"
    }
   },
   "outputs": [],
   "source": [
    "# 대상이 되는 주제 폴더 선택 (교육, 건강)\n",
    "#dirs = ['economy', 'society']\n",
    "dirs = ['education', 'health']\n",
    "\n",
    "# 기사에 출현하는 단어와 레이블을 저장할 리스트를 생성\n",
    "# 설명변수\n",
    "x_ls = [] \n",
    "# 목적변수\n",
    "y_ls = []\n",
    "\n",
    "tmp1 = []\n",
    "tmp2 = ''\n",
    "\n",
    "# 형태소 분석기 객체 생성\n",
    "#tokenizer_han = Hannanum()\n",
    "tokenizer = Kkma()\n",
    "\n",
    "# 각 폴더의 파일을 하나씩 읽어들이며, 전처리 후 리스트에 저장\n",
    "for i, d in enumerate(dirs):\n",
    "    # 파일 목록 읽어오기\n",
    "    files = os.listdir(dir_prefix + target_dir + '/' + d)\n",
    "    \n",
    "    for file in files:\n",
    "        # 각 파일을 읽어들이기\n",
    "        f = open(dir_prefix + target_dir + '/' + d + '/' + file, 'r', encoding='utf-8')\n",
    "        raw = f.read()  \n",
    "        \n",
    "        # 정규표현식을 사용하여 불필요한 문자열을 제거한 다음 파일 내용을 출력\n",
    "        reg_raw = re.sub(r'[-\\'@#:/◆▲0-9a-zA-Z<>!-\"*\\(\\)]', '', raw)\n",
    "        reg_raw = re.sub(r'[ ]+', ' ', reg_raw)\n",
    "        reg_raw = reg_raw.replace('\\n', ' ')\n",
    "        \n",
    "        # 형태소분석을 거쳐 명사만을 추출한 리스트를 생성\n",
    "        tokens = tokenizer.nouns(reg_raw)\n",
    "        \n",
    "        for token in tokens:\n",
    "            tmp1.append(token)\n",
    "            \n",
    "        tmp2 = ' '.join(tmp1)\n",
    "        x_ls.append(tmp2)\n",
    "        tmp1 = []\n",
    "        \n",
    "        # 기사 주제 레이블을 리스트에 저장\n",
    "        y_ls.append(i)\n",
    "        \n",
    "        # 파일 닫기\n",
    "        f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.670557Z",
     "start_time": "2019-09-28T00:36:52.721Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터프레임으로 변환해서 설명변수를 화면에 출력\n",
    "pd.DataFrame(x_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.672518Z",
     "start_time": "2019-09-28T00:36:52.725Z"
    }
   },
   "outputs": [],
   "source": [
    "# 첫 번째 기사로부터 추출한 단어를 출력\n",
    "print(x_ls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.673518Z",
     "start_time": "2019-09-28T00:36:52.727Z"
    }
   },
   "outputs": [],
   "source": [
    "# 목적변수를 화면에 출력\n",
    "print(y_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.675510Z",
     "start_time": "2019-09-28T00:36:52.738Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 설명변수와 목적변수를 NumPy 배열로 변환\n",
    "x_array = np.array(x_ls)\n",
    "y_array = np.array(y_ls)\n",
    "\n",
    "# 단어 출현 횟수를 계수\n",
    "cntvec = CountVectorizer()\n",
    "x_cntvecs = cntvec.fit_transform(x_array)\n",
    "x_cntarray = x_cntvecs.toarray()\n",
    "\n",
    "# 데이터프레임으로 단어 출현 횟수 출력\n",
    "pd.DataFrame(x_cntarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.677505Z",
     "start_time": "2019-09-28T00:36:52.767Z"
    }
   },
   "outputs": [],
   "source": [
    "# 단어와 단어의 인덱스 표시\n",
    "for k, v in sorted(cntvec.vocabulary_.items(), key=lambda x:x[1]):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.679499Z",
     "start_time": "2019-09-28T00:36:52.776Z"
    }
   },
   "outputs": [],
   "source": [
    "# 단어의 TF-IDF 계산\n",
    "tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "x_tfidf_vecs = tfidf_vec.fit_transform(x_array)\n",
    "x_tfidf_array = x_tfidf_vecs.toarray()\n",
    "\n",
    "# 데이터프레임으로 변환하여 단어의 출현 횟수를 출력\n",
    "pd.DataFrame(x_tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.680496Z",
     "start_time": "2019-09-28T00:36:52.780Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터를 훈련 데이터와 테스트 데이터로 분할\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(x_tfidf_array, y_array, test_size=0.2)\n",
    "\n",
    "# 데이터 건수 확인\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.682509Z",
     "start_time": "2019-09-28T00:36:52.804Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch 라이브러리 임포트\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.684486Z",
     "start_time": "2019-09-28T00:36:52.808Z"
    }
   },
   "outputs": [],
   "source": [
    "# 훈련 데이터 텐서 생성\n",
    "train_X = torch.from_numpy(train_X).float()\n",
    "train_Y = torch.from_numpy(train_Y).long()\n",
    "\n",
    "# 테스트 데이터 텐서 생성\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_Y = torch.from_numpy(test_Y).long()\n",
    "\n",
    "# 텐서로 변환한 데이터 건수 확인\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.686678Z",
     "start_time": "2019-09-28T00:36:52.811Z"
    }
   },
   "outputs": [],
   "source": [
    "# 설명변수와 목적변수의 텐서를 합친다\n",
    "train = TensorDataset(train_X, train_Y)\n",
    "\n",
    "# 첫번째 텐서 내용 확인\n",
    "print(train[0])\n",
    "\n",
    "# 미니배치 분할하기\n",
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.687478Z",
     "start_time": "2019-09-28T00:36:52.831Z"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망 구성\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(33572, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "# 인스턴스 생성\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모형 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.689472Z",
     "start_time": "2019-09-28T00:36:52.866Z"
    }
   },
   "outputs": [],
   "source": [
    "# 오차함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화 기법 선택\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# 학습\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 분할한 데이터(미니배치)를 차례로 꺼내옴\n",
    "    for train_x, train_y in train_loader:\n",
    "        # 계산 그래프 구성\n",
    "        train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "        # 경사 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 순전파 계산\n",
    "        output = model(train_x)\n",
    "        # 오차 계산\n",
    "        loss = criterion(output, train_y)\n",
    "        # 역전파 계산\n",
    "        loss.backward()\n",
    "        # 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        # 오차 누적 계산\n",
    "        total_loss += loss.data[0]\n",
    "        \n",
    "    # 100 에포크마다 누적오차를 출력\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:36:54.690470Z",
     "start_time": "2019-09-28T00:36:52.869Z"
    }
   },
   "outputs": [],
   "source": [
    "# 계산 그래프 구성\n",
    "test_x, test_y = Variable(test_X), Variable(test_Y)\n",
    "\n",
    "# 출력이 0 혹은 1이 되도록\n",
    "result = torch.max(model(test_x).data, 1)[1]\n",
    "\n",
    "# 모형의 정확도 계산\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "\n",
    "# 정확도 출력\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T00:37:23.324808Z",
     "start_time": "2019-09-28T00:37:23.293890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨: OS\n",
      " 볼륨 일련 번호: 56CE-D61E\n",
      "\n",
      " C:\\dev\\konlpy_tutorial 디렉터리\n",
      "\n",
      "2019-09-28  오전 09:35    <DIR>          .\n",
      "2019-09-28  오전 09:35    <DIR>          ..\n",
      "2019-09-28  오전 09:36    <DIR>          .idea\n",
      "2019-09-28  오전 01:03    <DIR>          .ipynb_checkpoints\n",
      "2019-09-27  오후 02:23           539,044 A New Hope.ipynb\n",
      "2019-09-27  오후 12:35            19,444 alice_mask.png\n",
      "2019-09-27  오후 03:43           230,450 baby-law-analysis.ipynb\n",
      "2019-09-27  오후 04:53    <DIR>          ch4\n",
      "2018-10-09  오후 07:26         1,108,354 Ch6-4_MLP_News.ipynb\n",
      "2019-09-27  오후 02:09    <DIR>          data\n",
      "2019-09-27  오후 10:07               811 gyu001.ipynb\n",
      "2019-09-27  오후 03:20            58,720 hangul-font.ipynb\n",
      "2019-09-27  오후 03:35            21,551 hangulfont-test.ipynb\n",
      "2019-09-28  오전 09:35               697 hkib.py\n",
      "2019-09-27  오후 05:24             5,652 identify language.ipynb\n",
      "2019-09-27  오후 05:41             5,848 identify language2.ipynb\n",
      "2019-09-27  오후 12:17            41,484 konlpy_kkma1.ipynb\n",
      "2019-09-27  오후 06:41       635,675,390 kowiki-latest-pages-articles.xml.bz2\n",
      "2019-09-27  오후 10:14               122 kyu_naver1.py\n",
      "2019-09-27  오후 03:00           102,652 law-analysis.ipynb\n",
      "2019-09-27  오후 03:08           100,169 matplotlib hangul font broken.ipynb\n",
      "2019-09-28  오전 01:20            12,004 naver_test.ipynb\n",
      "2019-09-27  오후 05:50             5,894 Okt.ipynb\n",
      "2016-06-27  오후 05:46        19,515,078 ratings.txt\n",
      "2016-06-27  오후 05:46         4,893,335 ratings_test.txt\n",
      "2016-06-27  오후 05:46        14,628,807 ratings_train.txt\n",
      "2019-09-27  오전 09:40                36 README.md\n",
      "2019-09-27  오후 04:35            27,642 sentence-similarity.ipynb\n",
      "2019-09-27  오후 10:51             7,140 sentiment_naver.ipynb\n",
      "2019-09-27  오후 03:56             6,943 simple-classifier-pos-neg-english.ipynb\n",
      "2019-09-27  오후 04:29            21,082 simple-classifier-pos-neg-korean.ipynb\n",
      "2019-09-27  오후 05:28    <DIR>          source_code\n",
      "2019-09-27  오후 05:36    <DIR>          test\n",
      "2019-09-27  오전 09:48                99 test1.txt\n",
      "2019-09-27  오후 12:22            58,098 tomato.jpg\n",
      "2019-09-27  오후 12:58            32,331 tomato2.jpg\n",
      "2019-09-27  오후 05:35    <DIR>          train\n",
      "2019-09-27  오후 03:28            26,221 Untitled.ipynb\n",
      "2019-09-27  오후 02:12         2,371,950 wordcloud.ipynb\n",
      "              30개 파일         679,517,048 바이트\n",
      "               9개 디렉터리  748,159,393,792 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
